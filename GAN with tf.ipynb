{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dhaneshkp/DesktopAssistant/blob/main/GAN%20with%20tf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Example data generation function\n",
        "def generate_data(num_samples=1000):\n",
        "    np.random.seed(0)\n",
        "    age = np.random.randint(20, 60, size=num_samples)\n",
        "    duration = np.random.randint(1, 20, size=num_samples)\n",
        "    risk_class = np.random.randint(0, 3, size=num_samples)\n",
        "    client_plan = np.random.randint(0, 4, size=num_samples)\n",
        "    premium_rate = age * 0.1 + duration * 0.05 + risk_class * 1.5 + client_plan * 0.2 + np.random.normal(0, 1, size=num_samples)\n",
        "\n",
        "    data = pd.DataFrame({\n",
        "        'age': age,\n",
        "        'duration': duration,\n",
        "        'risk_class': risk_class,\n",
        "        'client_plan': client_plan,\n",
        "        'premium_rate': premium_rate\n",
        "    })\n",
        "\n",
        "    # Introduce random missing values in premium_rate\n",
        "    missing_indices = np.random.choice(num_samples, size=int(0.1 * num_samples), replace=False)\n",
        "    data.loc[missing_indices, 'premium_rate'] = np.nan\n",
        "\n",
        "    return data\n",
        "\n",
        "# Data Preparation\n",
        "data = generate_data()\n",
        "features = data[['age', 'duration', 'risk_class', 'client_plan']].values\n",
        "premium_rates = data['premium_rate'].values\n",
        "\n",
        "# Normalize features and target\n",
        "scaler_X = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "features = scaler_X.fit_transform(features)\n",
        "premium_rates = premium_rates.reshape(-1, 1)\n",
        "premium_rates = scaler_y.fit_transform(premium_rates).reshape(-1)\n",
        "\n",
        "# Create mask for missing values\n",
        "mask = ~np.isnan(premium_rates)\n",
        "premium_rates[~mask] = 0  # Replace NaNs with zeros for now\n",
        "\n",
        "# Convert to tensors\n",
        "features = torch.tensor(features, dtype=torch.float32)\n",
        "premium_rates = torch.tensor(premium_rates, dtype=torch.float32).unsqueeze(1)\n",
        "mask = torch.tensor(mask, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "# Train a simple linear regression model\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(features[mask.squeeze().bool()], premium_rates[mask.squeeze().bool()])\n",
        "lin_reg_predictions = lin_reg.predict(features)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, feature_dim, target_dim):\n",
        "        super(Generator, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(feature_dim + target_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, target_dim),\n",
        "            nn.ReLU()  # Ensure non-negative outputs\n",
        "        )\n",
        "\n",
        "    def forward(self, x, noise):\n",
        "        input_combined = torch.cat((x, noise), dim=1)\n",
        "        return self.fc(input_combined)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, feature_dim, target_dim):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(feature_dim + target_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        input_combined = torch.cat((x, y), dim=1)\n",
        "        return self.fc(input_combined)\n",
        "\n",
        "# Initialize models\n",
        "feature_dim = features.shape[1]\n",
        "target_dim = 1  # Premium rate is a single value\n",
        "generator = Generator(feature_dim, target_dim)\n",
        "discriminator = Discriminator(feature_dim, target_dim)\n",
        "\n",
        "# Optimizers\n",
        "g_optimizer = optim.Adam(generator.parameters(), lr=0.001, betas=(0.5, 0.999))\n",
        "d_optimizer = optim.Adam(discriminator.parameters(), lr=0.001, betas=(0.5, 0.999))\n",
        "\n",
        "# Loss functions\n",
        "reconstruction_loss = nn.MSELoss()\n",
        "regression_loss = nn.MSELoss()\n",
        "\n",
        "# Gradient penalty\n",
        "def compute_gradient_penalty(discriminator, real_samples, fake_samples, real_features):\n",
        "    alpha = torch.rand(real_samples.size(0), 1)\n",
        "    alpha = alpha.expand(real_samples.size()).to(real_samples.device)\n",
        "\n",
        "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
        "    d_interpolates = discriminator(real_features, interpolates)\n",
        "\n",
        "    gradients = torch.autograd.grad(\n",
        "        outputs=d_interpolates,\n",
        "        inputs=interpolates,\n",
        "        grad_outputs=torch.ones(d_interpolates.size()).to(real_samples.device),\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "        only_inputs=True\n",
        "    )[0]\n",
        "\n",
        "    gradients = gradients.view(gradients.size(0), -1)\n",
        "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
        "    return gradient_penalty\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 1000\n",
        "batch_size = 64\n",
        "lambda_gp = 10  # Gradient penalty coefficient\n",
        "critic_iterations = 5  # Number of discriminator updates per generator update\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for _ in range(critic_iterations):\n",
        "        # Generate fake data\n",
        "        noise = torch.randn(batch_size, target_dim)\n",
        "\n",
        "        # Create batches (replace with your data loading logic)\n",
        "        batch_indices = np.random.choice(len(features), batch_size, replace=False)\n",
        "        real_features = features[batch_indices]\n",
        "        real_premium_rates = premium_rates[batch_indices]\n",
        "        batch_mask = mask[batch_indices]\n",
        "        lin_reg_preds = torch.tensor(lin_reg_predictions[batch_indices], dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "        # Generate fake targets using the generator\n",
        "        fake_premium_rates = generator(real_features, noise)\n",
        "\n",
        "        # Combine real and fake targets using the mask\n",
        "        combined_premium_rates = batch_mask * real_premium_rates + (1 - batch_mask) * fake_premium_rates\n",
        "\n",
        "        # Train Discriminator\n",
        "        d_optimizer.zero_grad()\n",
        "\n",
        "        real_validity = discriminator(real_features, real_premium_rates)\n",
        "        fake_validity = discriminator(real_features, combined_premium_rates.detach())\n",
        "\n",
        "        gradient_penalty = compute_gradient_penalty(discriminator, real_premium_rates, combined_premium_rates, real_features)\n",
        "        d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + lambda_gp * gradient_penalty\n",
        "\n",
        "        d_loss.backward()\n",
        "        d_optimizer.step()\n",
        "\n",
        "    # Train Generator\n",
        "    g_optimizer.zero_grad()\n",
        "\n",
        "    noise = torch.randn(batch_size, target_dim)\n",
        "    fake_premium_rates = generator(real_features, noise)\n",
        "    combined_premium_rates = batch_mask * real_premium_rates + (1 - batch_mask) * fake_premium_rates\n",
        "\n",
        "    g_loss_adv = -torch.mean(discriminator(real_features, combined_premium_rates))\n",
        "    g_loss_recon = reconstruction_loss(combined_premium_rates, real_premium_rates)\n",
        "    g_loss_reg = regression_loss(combined_premium_rates, lin_reg_preds)\n",
        "    g_loss = g_loss_adv + g_loss_recon + g_loss_reg\n",
        "\n",
        "    g_loss.backward()\n",
        "    g_optimizer.step()\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f'Epoch [{epoch}/{num_epochs}], d_loss: {d_loss.item()}, g_loss: {g_loss.item()}')\n",
        "\n",
        "# Use the trained generator for imputation\n",
        "noise = torch.randn(len(features), target_dim)\n",
        "imputed_premium_rates = generator(features, noise).detach().numpy()\n",
        "imputed_premium_rates = mask.numpy() * premium_rates.numpy() + (1 - mask.numpy()) * imputed_premium_rates\n",
        "\n",
        "# Denormalize the imputed values\n",
        "imputed_premium_rates = scaler_y.inverse_transform(imputed_premium_rates)\n",
        "\n",
        "# Replace the original missing values with the imputed values\n",
        "data['imputed_remium_rate'] = imputed_premium_rates\n",
        "print(data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZLG6Cn2LXEQ",
        "outputId": "3f02b2a5-bbee-4982-aeb7-dfd0231db025"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([64, 1, 1])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [0/1000], d_loss: 7.523110389709473, g_loss: 1.493195652961731\n",
            "Epoch [100/1000], d_loss: 0.03502834215760231, g_loss: 1.0470221042633057\n",
            "Epoch [200/1000], d_loss: 0.030421800911426544, g_loss: 1.1214067935943604\n",
            "Epoch [300/1000], d_loss: 0.019457165151834488, g_loss: 1.5163860321044922\n",
            "Epoch [400/1000], d_loss: 0.018131723627448082, g_loss: 1.1728957891464233\n",
            "Epoch [500/1000], d_loss: 0.017252637073397636, g_loss: 1.4371570348739624\n",
            "Epoch [600/1000], d_loss: 0.013920199126005173, g_loss: 1.1134288311004639\n",
            "Epoch [700/1000], d_loss: 0.013138755224645138, g_loss: 1.176929235458374\n",
            "Epoch [800/1000], d_loss: 0.010633702389895916, g_loss: 1.4720985889434814\n",
            "Epoch [900/1000], d_loss: 0.013439767062664032, g_loss: 1.284769058227539\n",
            "     age  duration  risk_class  client_plan  premium_rate  imputed_remium_rate\n",
            "0     20         5           2            0      6.120132             6.120131\n",
            "1     23         2           2            2      6.453851             6.453850\n",
            "2     23        18           2            0      4.883036             4.883036\n",
            "3     59        18           0            3      8.344990             8.344990\n",
            "4     29        17           0            2           NaN             6.251120\n",
            "..   ...       ...         ...          ...           ...                  ...\n",
            "995   31         6           0            0      3.637034             3.637035\n",
            "996   33         9           0            0      4.123255             4.123255\n",
            "997   49         6           1            3           NaN             6.251120\n",
            "998   51         5           1            0      7.780815             7.780815\n",
            "999   38        12           0            2      4.200906             4.200905\n",
            "\n",
            "[1000 rows x 6 columns]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "TensorFlow with GPU",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}